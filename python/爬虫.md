1. **urllib爬取网页**
    import urllib.request
    url = ''
    file = urllib.request.urlopen(url)
    data = file.read()
    `file.readlines()` 读取文件的全部内容，把读取到的内容赋值给一个变量列表

2. **爬取网页并写入本地**  
   `urllib.request.urlretrieve(url, filename=本地文件地址)`  
   **urlretrieve**执行的过程中会产生一些缓存，可以使用**urlcleanup**进行清除。像这样：`urllib.request.urlcleanup`  

3. **返回与当前环境有关的信息**  
    `file.info()`  
    调用格式为`爬取的网页.info()`  
    * **获取爬取的网页的状态码**  
    调用格式为`爬取的网页.getcode()`若返回200为正确
    * **获取当前爬取网页的URL**  
    `file.geturl`  
    调用格式为`爬取的网页.geturl`  
### 编码与解码
* **编码**  
一般来说，URL标准中只会允许一部分ASCll字符，比如数字、字母、部分符号等，而其他一些字符如汉字，是不符合URL标准的，此时需要进行URL编码方可解决。比如在URL中输入中文或者“:”或者“&”等不符合标准的字符时，需要进行编码。  
我们可以使用`urllib.request.quote()`进行编码。 如：  
`urllib.request.quote("http://www.sina.com.cn")`编码结果为`http%3A//www.sina.com.cn`
* **解码**  
`urllib.request.unquote(url)`

### 模拟浏览器
* **方法一：使用build_opener()修改报头**  
```python
headers = ('User-Agent','') # 存储User-Agent信息，定义格式为('User-Agent',具体信息)
opener = urllib.request.build_opener() # 创建自定义的opener对象
opener.addheaders = [headers] # 设置头信息，设置格式为：opener对象名.addheaders=[头信息]。还可以设置其他多种HTTP请求头信息。
data = opener.open(url).read() # 使用opener对象模拟浏览器打开网址
# 可以设置超时：data = opener.open(url,timeout=10).read()
```
* **方法二：使用add_header()添加报头**
```python
req = urllib.request.Request(url) # 创建Request对象，格式为：urllib.request.Request(url地址)
req.add_header('User-Agent', 字段值) # 添加报头， 格式为：Request对象名.add_header(字段名,字段值)
data = urllib.request.urlopen(req).read() # 打开对应网址并读取网页内容
# 可以设置超时：data = urllib.request.urlopen(req,timeout=10).read()
```

### HTTP协议请求
如果要进行客户端与服务器之间的消息传递，我们可以使用HTTP协议请求进行。  
HTTP协议请求主要分为6中类型，各类型的主要作用如下：  
1. **GET请求**  
   GET请求通过URL网址传递信息，可以直接在URL中写上要传递的信息，也可以由表单进行传递。如果使用表单进行传递，这表单中的信息会自动转化为URL地址中的数据，通过URL进行传递。  
2. **POST请求**  
   可以向服务器提交数据，是一种比较主流也比较安全的数据传递方式，比如在登录时，经常使用POST传递数据。  
3. **PUT请求**
   请求服务器存储一个资源，通常要指定存储的位置。  
4. **DELETE请求**  
   请求服务器删除一个资源。
5. **HEAD**  
   请求获取对应的HTTP报头信息。
6. **OPTIONS**  
   可以获得当前URL所支持的类型。  
7. 除此之外，还有TRACE请求与CONNECT请求等，TRACE主要用于测试或诊断，用得非常少。

### 代理服务器
* **设置代理服务器示例**  
  
