# 决策树（Decision Tree）

## 1 简介

决策树是一种用于解决分类和回归问题的机器学习算法，它通过构建一棵树状结构，将数据集分成多个子集，每个子集对应一个叶节点。

## 2 不纯性度量

### 2.1 单结点的不纯性度量

结点的不纯性度量，即度量共有结点的数据实例的类别标签的差异程度。以下是几种可用于评估结点 $t$ 的不纯性度量的方法：
$$熵=-\sum_{i=1}^c p_i \log_2 p_i \tag{1}$$

$$基尼指数=1-\sum_{i=1}^c p_i^2 \tag{2}$$

$$分类误差=1-\max_{i=1}^c p_i \tag{3}$$

其中 $p_i$ 是结点 $t$ 中第 $i$ 类样本的比例，$c$ 是类别数，计算熵时 $0\log_2 0=0$。

### 2.2 子结点的集合不纯性

$$I(子结点) = \sum_{i=1}^k \frac{N_i}{N} I(子结点_i) \tag{4}$$

其中 $N_i$ 是子结点 $i$ 中样本的数量，$N$ 是父结点中样本的数量，$I(子结点_i)$ 是子结点 $i$ 的不纯性度量，$k$ 是子结点的数量。

### 2.3 纯度增益

$$G(t) = I(父结点) - I(子结点) \tag{5}$$

对于熵和基尼指数，**纯度增益是非负的**。
当用**熵**当做不纯性度量时，熵的增益通常称为**信息增益**（Information Gain）。

### 2.4 增益率

熵和基尼指数等不纯性度量存在一个潜在的局限，即它们更容易选择具有大量不同值的定性属性。为了克服这个局限，一种方法是仅生成**二元决策树**，决策树分类器 **CART** 就是使用这种方法。另一种方法修改划分标准，使用**增益率**（Gain Ratio）来选择划分属性，如 **ID3** 及其后续版本 **C4.5** 就是使用这种方法。

$$增益率=\frac{\Delta信息}{分裂信息} = \frac{I(父结点)-I(子结点)}{-\sum_{i=1}^k w_i \log_2 w_i} \tag{6}$$

其中 $w_i=\frac{N_i}{N}$ 是子结点 $i$ 中样本的比例，$k$ 是子结点的数量。

## 3 决策树构建

下面给出伪代码，其中 $E$ 是训练实例，$F$ 是属性集。

```python
if stopping_condition(E, F) = true then
    leaf = createNode()
    leaf.label = classify(E)
    return leaf
else
    root = createNode()
    root.test_condition = choose_test_attribute(E, F)
    for each condition c of root.test_condition do
        E_c = [e in E | e[root.test_condition] = c]
        F_c = F - {root.test_condition}
        subtree = build_tree(E_c, F_c)
        root.children[c] = subtree
    return root
```
