# 支持向量机（Support Vector Machine, SVM）

## 1 简介

支持向量机是一种用于解决分类问题的机器学习算法，它通过在特征空间中找到一个最优的超平面，将不同类别的数据分开。对于线性不可分数据，可通过核技巧隐式映射到高维空间。硬间隔SVM假设数据线性可分，仅使用训练实例中最难分类的一个子集（支持向量）来定义决策边界。

## 2 线性SVM（硬间隔最大间隔分类器）

### 2.1 数学推导

考虑一个二分类问题，标签 $y_i \in \{1, -1\}$，分离超平面为 $\mathbf{w}^T \mathbf{x} + b = 0$。正确分类要求：
$$
\begin{aligned}
\mathbf{w}^T \mathbf{x}_i + b \geq 0, \quad &y_i = 1 \\
\mathbf{w}^T \mathbf{x}_i + b < 0, \quad &y_i = -1
\end{aligned}
$$
点到超平面的几何距离为：
$$
D(\mathbf{x}) = \frac{|\mathbf{w}^T \mathbf{x} + b|}{\|\mathbf{w}\|}
$$
函数间隔（functional margin）定义为 $y_i (\mathbf{w}^T \mathbf{x}_i + b)$，几何间隔（geometric margin）为函数间隔除以 $\|\mathbf{w}\|$。

为找到最大间隔超平面，我们希望两类样本到超平面的最近距离（间隔）最大。令所有样本的函数间隔不小于 $\gamma$：
$$
y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq \gamma, \quad i=1,\dots,n
$$
此时几何间隔为 $\frac{\gamma}{\|\mathbf{w}\|}$。

由于 $\mathbf{w}$ 和 $b$ 可以同比例缩放而不改变超平面，若某个 $(\mathbf{w}, b)$ 满足约束，则 $k\mathbf{w}, kb$（$k>0$）也满足。我们可以固定支持向量上的函数间隔为1，即取 $\gamma = 1$（通过缩放实现），则几何间隔变为 $\frac{1} {\|\mathbf{w}\|}$。

最大化几何间隔等价于：
$$
\max_{\mathbf{w}, b} \frac{1}{\|\mathbf{w}\|} \quad \text{s.t.} \quad y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1, \quad i=1,\dots,n
$$
等价于以下优化问题（常数2不影响最优解）：
$$
\min_{\mathbf{w}, b} \frac{1}{2} \|\mathbf{w}\|^2 \quad \text{s.t.} \quad y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1, \quad i=1,\dots,n \tag{5}
$$

### 2.2 拉格朗日对偶推导

引入拉格朗日乘子 $\lambda_i \geq 0$，构造拉格朗日函数（标准形式）：

$$
L(\mathbf{w}, b, \lambda) = \frac{1}{2} \|\mathbf{w}\|^2 + \sum_{i=1}^n \lambda_i \left(1 - y_i (\mathbf{w}^T \mathbf{x}_i + b)\right)
$$

原始问题是

$$\min_{\mathbf{w}, b} \max_{\lambda \geq 0} L(\mathbf{w}, b, \lambda)$$

由于问题满足Slater条件，存在强对偶性，可转为对偶问题：
$$
\max_{\lambda \geq 0} \min_{\mathbf{w}, b} L(\mathbf{w}, b, \lambda)
$$
先对 $\mathbf{w}, b$ 求偏导并令为0：
$$
\frac{\partial L}{\partial \mathbf{w}} = \mathbf{w} - \sum_{i=1}^n \lambda_i y_i \mathbf{x}_i = 0 \quad \Rightarrow \quad \mathbf{w} = \sum_{i=1}^n \lambda_i y_i \mathbf{x}_i
$$
$$
\frac{\partial L}{\partial b} = -\sum_{i=1}^n \lambda_i y_i = 0 \quad \Rightarrow \quad \sum_{i=1}^n \lambda_i y_i = 0
$$
将以上关系代入 $L$，消去 $\mathbf{w}, b$，得到：
$$
\min_{\mathbf{w}, b} L = \sum_{i=1}^n \lambda_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \lambda_i \lambda_j y_i y_j \mathbf{x}_i^T \mathbf{x}_j
$$
因此对偶优化问题为：
$$
\max_{\lambda} \left[ \sum_{i=1}^n \lambda_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \lambda_i \lambda_j y_i y_j \mathbf{x}_i^T \mathbf{x}_j \right]
$$
$$
\text{s.t.} \quad \sum_{i=1}^n \lambda_i y_i = 0, \quad \lambda_i \geq 0, \quad i=1,\dots,n
$$
等价于最小化二次项（常用形式）：
$$
\min_{\lambda} \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \lambda_i \lambda_j y_i y_j \mathbf{x}_i^T \mathbf{x}_j - \sum_{i=1}^n \lambda_i
$$
$$
\text{s.t.} \quad \sum_{i=1}^n \lambda_i y_i = 0, \quad \lambda_i \geq 0
$$

### 2.3 支持向量与KKT条件

- 最优解中 $\lambda_i > 0$ 的样本称为**支持向量**，它们位于间隔边界上：$y_i (\mathbf{w}^T \mathbf{x}_i + b) = 1$。
- 非支持向量对应 $\lambda_i = 0$。
- 最优 $\mathbf{w} = \sum_{i=1}^n \lambda_i y_i \mathbf{x}_i$（仅由支持向量决定）。
- $b$ 可通过任意支持向量计算：$b = y_k - \mathbf{w}^T \mathbf{x}_k$（实际中常取多个支持向量平均以提高数值稳定性）。

---

*更新于 2025-12-11 15:18*
